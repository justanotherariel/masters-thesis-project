{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper, RGBImgObsWrapper\n",
    "from src.modules.environment.minigrid_wrappers import FullyObsWrapper\n",
    "import numpy as np\n",
    "\n",
    "# minigrid.register_minigrid_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [10  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 8  1  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]]]\n",
      "[[[ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [10  0  3]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 8  1  0]\n",
      "  [ 2  5  0]]\n",
      "\n",
      " [[ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]\n",
      "  [ 2  5  0]]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "env = FullyObsWrapper(env)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "print(observation)\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(0)\n",
    "print(observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([252, 128, 1, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "requested an output size of torch.Size([5, 5]), but valid sizes range from [7, 7] to [8, 8] (for an input of torch.Size([4, 4]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_enc\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# torch.Size([252, 128, 1, 1])\u001b[39;00m\n\u001b[1;32m     65\u001b[0m input_dec \u001b[38;5;241m=\u001b[39m decode(input_enc)\n\u001b[0;32m---> 66\u001b[0m input_dec \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_dec\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# torch.Size([252, 32, 4, 4])\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Sample Code from Torch Documentation\u001b[39;00m\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:944\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 944\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_padding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_spatial_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv_transpose2d(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    950\u001b[0m     output_padding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:655\u001b[0m, in \u001b[0;36m_ConvTransposeNd._output_padding\u001b[0;34m(self, input, output_size, stride, padding, kernel_size, num_spatial_dims, dilation)\u001b[0m\n\u001b[1;32m    653\u001b[0m     max_size \u001b[38;5;241m=\u001b[39m max_sizes[i]\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m min_size \u001b[38;5;129;01mor\u001b[39;00m size \u001b[38;5;241m>\u001b[39m max_size:\n\u001b[0;32m--> 655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested an output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but valid sizes range \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (for an input of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    659\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_spatial_dims):\n",
      "\u001b[0;31mValueError\u001b[0m: requested an output size of torch.Size([5, 5]), but valid sizes range from [7, 7] to [8, 8] (for an input of torch.Size([4, 4]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "\n",
    "from src.modules.training.datasets.utils import TokenIndex\n",
    "from src.modules.training.models.cnn import CNN\n",
    "\n",
    "model = CNN()\n",
    "info = {\n",
    "    'env_build': {\n",
    "        'observation_space': spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(5, 5, 3),\n",
    "            dtype=\"uint8\",\n",
    "        ),\n",
    "        'action_space': spaces.Discrete(7),\n",
    "    },\n",
    "    'token_index': TokenIndex({\n",
    "        'observation': [(0, 10), (1, 6), (2, 3), (3,5)],\n",
    "        'action': [(0, 7)],\n",
    "        'reward': [(0, 0)],\n",
    "    }),\n",
    "}\n",
    "model.setup(info)\n",
    "\n",
    "hidden_dims: list[int] = [32, 64, 128]\n",
    "reversed_dims = list(reversed(hidden_dims))\n",
    "decode = nn.Sequential(\n",
    "    nn.ConvTranspose2d(\n",
    "            reversed_dims[0],\n",
    "            reversed_dims[1],\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        ),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(reversed_dims[1]),\n",
    "    nn.ConvTranspose2d(\n",
    "            reversed_dims[1],\n",
    "            reversed_dims[2],\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        ),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(reversed_dims[2]),\n",
    ")\n",
    "\n",
    "final_decoder = nn.ConvTranspose2d(\n",
    "    reversed_dims[2],\n",
    "    24,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=1\n",
    ")\n",
    "\n",
    "input = torch.randn(252, 24, 5, 5)\n",
    "input_enc = model.encode(input)\n",
    "print(input_enc.shape) # torch.Size([252, 128, 1, 1])\n",
    "\n",
    "input_dec = decode(input_enc)\n",
    "\n",
    "# ValueError: requested an output size of torch.Size([5, 5]), but valid sizes range from [7, 7] to [8, 8] (for an input of torch.Size([4, 4]))\n",
    "# input_dec = final_decoder(input_dec, output_size=input.size())\n",
    "\n",
    "print(input_dec.shape) # torch.Size([252, 32, 4, 4])\n",
    "\n",
    "\n",
    "# Sample Code from Torch Documentation\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "print(input.size()) # torch.Size([1, 16, 12, 12])\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "print(h.size()) # torch.Size([1, 16, 6, 6])\n",
    "output = upsample(h)\n",
    "print(output.size()) # torch.Size([1, 16, 11, 11])\n",
    "output = upsample(h, output_size=input.size())\n",
    "print(output.size()) # torch.Size([1, 16, 12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([252, 128, 1, 1])\n",
      "torch.Size([252, 64, 2, 2])\n",
      "torch.Size([252, 32, 3, 3])\n",
      "torch.Size([252, 24, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Setup Network: Encoder\n",
    "hidden_dims: list[int] = [32, 64, 128]\n",
    "encode = nn.Sequential(\n",
    "    nn.Conv2d(24, hidden_dims[0], kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(hidden_dims[0]),\n",
    "    nn.Conv2d(hidden_dims[0], hidden_dims[1], kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(hidden_dims[1]),\n",
    "    nn.Conv2d(hidden_dims[1], hidden_dims[2], kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(hidden_dims[2]),\n",
    ")\n",
    "\n",
    "# Setup Network: Decoder\n",
    "reversed_dims = list(reversed(hidden_dims))\n",
    "relu = nn.ReLU()\n",
    "deconv1 = nn.ConvTranspose2d(\n",
    "    reversed_dims[0],\n",
    "    reversed_dims[1],\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=1\n",
    ")\n",
    "bn1 = nn.BatchNorm2d(reversed_dims[1])\n",
    "deconv2 = nn.ConvTranspose2d(\n",
    "    reversed_dims[1],\n",
    "    reversed_dims[2],\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=0\n",
    ")\n",
    "bn2 = nn.BatchNorm2d(reversed_dims[2])\n",
    "deconv3 = nn.ConvTranspose2d(\n",
    "    reversed_dims[2],\n",
    "    24,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=0\n",
    ")\n",
    "\n",
    "# Input\n",
    "input = torch.randn(252, 24, 5, 5)\n",
    "\n",
    "# Encode\n",
    "input_enc = encode(input)\n",
    "print(input_enc.shape) # torch.Size([252, 128, 1, 1])\n",
    "\n",
    "# Decode\n",
    "input_dec = relu(bn1(deconv1(input_enc)))\n",
    "print(input_dec.shape) \n",
    "input_dec = relu(bn2(deconv2(input_dec)))\n",
    "print(input_dec.shape)\n",
    "input_dec = deconv3(input_dec)\n",
    "print(input_dec.shape)\n",
    "\n",
    "\n",
    "# # Sample Code from Torch Documentation\n",
    "# input = torch.randn(1, 16, 12, 12)\n",
    "# print(input.size()) # torch.Size([1, 16, 12, 12])\n",
    "# downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "# upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "# h = downsample(input)\n",
    "# print(h.size()) # torch.Size([1, 16, 6, 6])\n",
    "# output = upsample(h)\n",
    "# print(output.size()) # torch.Size([1, 16, 11, 11])\n",
    "# output = upsample(h, output_size=input.size())\n",
    "# print(output.size()) # torch.Size([1, 16, 12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([252, 24, 5, 5])\n",
      "Output shape: torch.Size([252, 24, 5, 5])\n",
      "Shapes match: True\n",
      "Encoded shape: torch.Size([252, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(24, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.dbn1 = nn.BatchNorm2d(64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.dbn2 = nn.BatchNorm2d(32)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 24, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Save input size for later\n",
    "        input_size = x.size()\n",
    "        \n",
    "        # Encoder\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Decoder with dynamic output padding\n",
    "        x = F.relu(self.dbn1(self.deconv1(x, output_size=(input_size[0], 64, 2, 2))))\n",
    "        x = F.relu(self.dbn2(self.deconv2(x, output_size=(input_size[0], 32, 3, 3))))\n",
    "        x = self.deconv3(x, output_size=input_size)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and test\n",
    "model = Autoencoder()\n",
    "input = torch.randn(252, 24, 5, 5)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "    \n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Shapes match:\", input.shape == output.shape)\n",
    "\n",
    "# Optional: Test the encoding part separately\n",
    "with torch.no_grad():\n",
    "    # Manual encoding\n",
    "    x = input\n",
    "    x = F.relu(model.bn1(model.conv1(x)))\n",
    "    x = F.relu(model.bn2(model.conv2(x)))\n",
    "    encoded = F.relu(model.bn3(model.conv3(x)))\n",
    "    print(\"Encoded shape:\", encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with input_size=5, hidden_dims=[32, 64, 128]\n",
      "\n",
      "Model structure:\n",
      "Encoder layers: 3\n",
      "Decoder layers: 3\n",
      "\n",
      "Encoding shapes:\n",
      "Input: torch.Size([252, 24, 5, 5])\n",
      "After encoder 1: torch.Size([252, 32, 3, 3])\n",
      "After encoder 2: torch.Size([252, 64, 2, 2])\n",
      "After encoder 3: torch.Size([252, 128, 1, 1])\n",
      "Intermediate sizes: [torch.Size([252, 32, 3, 3]), torch.Size([252, 64, 2, 2]), torch.Size([252, 128, 1, 1])]\n",
      "intermediate_sizes[-i-1 = -1]: torch.Size([252, 128, 1, 1])\n",
      "intermediate_sizes[-i-1 = -2]: torch.Size([252, 64, 2, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_BatchNorm.forward() got an unexpected keyword argument 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Test different configurations\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtest_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m test_autoencoder(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, hidden_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m])\n\u001b[1;32m    113\u001b[0m test_autoencoder(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, hidden_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m])\n",
      "Cell \u001b[0;32mIn[24], line 103\u001b[0m, in \u001b[0;36mtest_autoencoder\u001b[0;34m(input_size, hidden_dims)\u001b[0m\n\u001b[1;32m    100\u001b[0m encoded \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 68\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m output_size \u001b[38;5;241m=\u001b[39m intermediate_sizes[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Apply transposed conv with specific output size\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Apply BatchNorm and ReLU\u001b[39;00m\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m](x))\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/master/master-thesis/project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _BatchNorm.forward() got an unexpected keyword argument 'output_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 24, hidden_dims: List[int] = [32, 64, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save configuration\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        in_channels = input_channels\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.encoder_layers.extend([\n",
    "                nn.Conv2d(in_channels, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            in_channels = hidden_dim\n",
    "            \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        reversed_dims = list(reversed(hidden_dims))\n",
    "        \n",
    "        for i in range(len(reversed_dims) - 1):\n",
    "            self.decoder_layers.extend([\n",
    "                nn.ConvTranspose2d(reversed_dims[i], reversed_dims[i + 1], \n",
    "                                 kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(reversed_dims[i + 1]),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            \n",
    "        # Final decoder layer (no BatchNorm or ReLU)\n",
    "        self.decoder_layers.append(\n",
    "            nn.ConvTranspose2d(reversed_dims[-1], input_channels, \n",
    "                             kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Store input size\n",
    "        input_size = x.size()\n",
    "        \n",
    "        # Store intermediate sizes for decoder\n",
    "        intermediate_sizes = []\n",
    "        \n",
    "        # Encoding\n",
    "        for i in range(0, len(self.encoder_layers), 3):\n",
    "            # Apply conv\n",
    "            x = self.encoder_layers[i](x)\n",
    "            # Store size after conv\n",
    "            intermediate_sizes.append(x.size())\n",
    "            # Apply BatchNorm and ReLU\n",
    "            x = self.encoder_layers[i + 2](self.encoder_layers[i + 1](x))\n",
    "        \n",
    "        print(\"Intermediate sizes:\", intermediate_sizes)\n",
    "        \n",
    "        # Decoding\n",
    "        for i in range(0, len(self.decoder_layers) - 1):\n",
    "            # Get corresponding size from encoding phase\n",
    "            print(f'intermediate_sizes[-i-1 = {-i-1}]: {intermediate_sizes[-i-1]}')\n",
    "            output_size = intermediate_sizes[-i-1]\n",
    "            # Apply transposed conv with specific output size\n",
    "            x = self.decoder_layers[i](x, output_size=output_size)\n",
    "            # Apply BatchNorm and ReLU\n",
    "            x = self.decoder_layers[i + 2](self.decoder_layers[i + 1](x))\n",
    "            \n",
    "        # Final decoder layer\n",
    "        x = self.decoder_layers[-1](x, output_size=input_size)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def test_autoencoder(input_size: int, hidden_dims: List[int]):\n",
    "    print(f\"\\nTesting with input_size={input_size}, hidden_dims={hidden_dims}\")\n",
    "    \n",
    "    model = Autoencoder(input_channels=24, hidden_dims=hidden_dims)\n",
    "    input = torch.randn(252, 24, input_size, input_size)\n",
    "    \n",
    "    # Print model structure\n",
    "    print(\"\\nModel structure:\")\n",
    "    print(\"Encoder layers:\", len(model.encoder_layers)//3)\n",
    "    print(\"Decoder layers:\", len(model.decoder_layers)//3 + 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Track shapes through encoding\n",
    "        x = input\n",
    "        print(f\"\\nEncoding shapes:\")\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        \n",
    "        for i in range(0, len(model.encoder_layers), 3):\n",
    "            x = model.encoder_layers[i](x)\n",
    "            x = model.encoder_layers[i + 2](model.encoder_layers[i + 1](x))\n",
    "            print(f\"After encoder {i//3 + 1}: {x.shape}\")\n",
    "        \n",
    "        encoded = x\n",
    "        \n",
    "        # Decode\n",
    "        output = model(input)\n",
    "        \n",
    "        print(f\"\\nFinal shapes:\")\n",
    "        print(f\"Encoded: {encoded.shape}\")\n",
    "        print(f\"Output: {output.shape}\")\n",
    "        print(f\"Shapes match: {input.shape == output.shape}\")\n",
    "\n",
    "# Test different configurations\n",
    "test_autoencoder(input_size=5, hidden_dims=[32, 64, 128])\n",
    "test_autoencoder(input_size=9, hidden_dims=[16, 32, 64, 128])\n",
    "test_autoencoder(input_size=16, hidden_dims=[32, 64])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
