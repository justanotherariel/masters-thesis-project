{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "\n",
    "from src.modules.training.datasets.utils import TokenIndex\n",
    "from src.modules.training.models.cnn import CNN\n",
    "\n",
    "model = CNN()\n",
    "info = {\n",
    "    'env_build': {\n",
    "        'observation_space': spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(5, 5, 3),\n",
    "            dtype=\"uint8\",\n",
    "        ),\n",
    "        'action_space': spaces.Discrete(7),\n",
    "    },\n",
    "    'token_index': TokenIndex({\n",
    "        'observation': [(0, 10), (1, 6), (2, 3), (3,5)],\n",
    "        'action': [(0, 7)],\n",
    "        'reward': [(0, 0)],\n",
    "    }),\n",
    "}\n",
    "model.setup(info)\n",
    "\n",
    "hidden_dims: list[int] = [32, 64, 128]\n",
    "reversed_dims = list(reversed(hidden_dims))\n",
    "decode = nn.Sequential(\n",
    "    nn.ConvTranspose2d(\n",
    "            reversed_dims[0],\n",
    "            reversed_dims[1],\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        ),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(reversed_dims[1]),\n",
    "    nn.ConvTranspose2d(\n",
    "            reversed_dims[1],\n",
    "            reversed_dims[2],\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        ),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(reversed_dims[2]),\n",
    ")\n",
    "\n",
    "final_decoder = nn.ConvTranspose2d(\n",
    "    reversed_dims[2],\n",
    "    24,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=1\n",
    ")\n",
    "\n",
    "input = torch.randn(252, 24, 5, 5)\n",
    "input_enc = model.encode(input)\n",
    "print(input_enc.shape) # torch.Size([252, 128, 1, 1])\n",
    "\n",
    "input_dec = decode(input_enc)\n",
    "\n",
    "# ValueError: requested an output size of torch.Size([5, 5]), but valid sizes range from [7, 7] to [8, 8] (for an input of torch.Size([4, 4]))\n",
    "# input_dec = final_decoder(input_dec, output_size=input.size())\n",
    "\n",
    "print(input_dec.shape) # torch.Size([252, 32, 4, 4])\n",
    "\n",
    "\n",
    "# Sample Code from Torch Documentation\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "print(input.size()) # torch.Size([1, 16, 12, 12])\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "print(h.size()) # torch.Size([1, 16, 6, 6])\n",
    "output = upsample(h)\n",
    "print(output.size()) # torch.Size([1, 16, 11, 11])\n",
    "output = upsample(h, output_size=input.size())\n",
    "print(output.size()) # torch.Size([1, 16, 12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_channels: int = 24\n",
    "hidden_channels: list[int] = [32, 64]\n",
    "\n",
    "relu = nn.ReLU()\n",
    "channels = [input_channels] + hidden_channels\n",
    "reversed_channels = list(reversed(channels))\n",
    "\n",
    "# Setup Network: Encoder\n",
    "## Encoder 0\n",
    "encode00 = nn.Conv2d(channels[0], channels[1], 3, padding=1)\n",
    "enc_bn00 = nn.BatchNorm2d(channels[1])\n",
    "encode01 = nn.Conv2d(channels[1], channels[1], 3, padding=1)\n",
    "enc_bn01 = nn.BatchNorm2d(channels[1])\n",
    "\n",
    "## Encoder 1\n",
    "mp01 = nn.MaxPool2d(2)\n",
    "encode10 = nn.Conv2d(channels[1], channels[2], 3, padding=1)\n",
    "enc_bn10 = nn.BatchNorm2d(channels[2])\n",
    "encode11 = nn.Conv2d(channels[2], channels[2], 3, padding=1)\n",
    "enc_bn11 = nn.BatchNorm2d(channels[2])\n",
    "\n",
    "# Setup Network: Decoder\n",
    "## Decoder 1\n",
    "deconv_up_01 = nn.ConvTranspose2d(reversed_channels[0], reversed_channels[0]//2, kernel_size=2, stride=2)\n",
    "deconv_bn1 = nn.BatchNorm2d(reversed_channels[1])\n",
    "deconv2 = nn.ConvTranspose2d(reversed_channels[1], reversed_channels[2], 3, padding=1)\n",
    "bn2 = nn.BatchNorm2d(reversed_channels[2])\n",
    "\n",
    "# Input\n",
    "input = torch.randn(252, 24, 5, 5)\n",
    "print(f'input  : {input.shape}')\n",
    "\n",
    "# Encode\n",
    "input_enc = relu(bn01(encode1(input)))\n",
    "print(f'encode1: {input_enc.shape}')\n",
    "input_enc = relu(bn02(encode2(input_enc)))\n",
    "print(f'encode2: {input_enc.shape}')\n",
    "input_enc = mp1(input_enc)\n",
    "print(f'maxpool: {input_enc.shape}')\n",
    "\n",
    "# Decode\n",
    "input_dec = relu(bn1(deconv1(input_enc)))\n",
    "print(f'decode1: {input_dec.shape}')\n",
    "input_dec = relu(bn2(deconv2(input_dec)))\n",
    "print(f'decode2: {input_dec.shape}')\n",
    "\n",
    "# # Sample Code from Torch Documentation\n",
    "# input = torch.randn(1, 16, 12, 12)\n",
    "# print(input.size()) # torch.Size([1, 16, 12, 12])\n",
    "# downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "# upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "# h = downsample(input)\n",
    "# print(h.size()) # torch.Size([1, 16, 6, 6])\n",
    "# output = upsample(h)\n",
    "# print(output.size()) # torch.Size([1, 16, 11, 11])\n",
    "# output = upsample(h, output_size=input.size())\n",
    "# print(output.size()) # torch.Size([1, 16, 12, 12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
