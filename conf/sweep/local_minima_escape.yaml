command:
  - python3
  - ${program}
  - ${args_no_hyphens}
method: bayes
project: Thesis
entity: a-ebersberger-tu-delft
program: train.py
name: Local Minima Escape | Transformer
metric:
  name: Train/Agent Accuracy
  goal: maximize
parameters:

  model:
    value: transformer_extensive

  # n_trials:
  #   value: 5

  # Optimizer/Scheduler Parameters
  model.train_sys.steps.0.optimizer._args_.0.path:
    values:
      - torch.optim.Adam
      - torch.optim.AdamW
      - torch.optim.RAdam
      - torch.optim.NAdam
      - torch.optim.Adafactor

  model.train_sys.steps.0.optimizer.lr:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.1

  # StepLR Scheduler
  model.train_sys.steps.0.scheduler.decay_rate:
    distribution: uniform
    min: 0.2
    max: 0.9

  model.train_sys.steps.0.scheduler.decay_t:
    distribution: int_uniform
    min: 50
    max: 300

  # CosineLR Scheduler
  # model.train_sys.steps.0.scheduler.t_initial:
  #   distribution: int_uniform
  #   min: 50
  #   max: 300

  # model.train_sys.steps.0.scheduler.lr_min:
  #   distribution: log_uniform_values
  #   min: 1e-6
  #   max: 1e-4

  # model.train_sys.steps.0.scheduler.cycle_mul:
  #   distribution: uniform
  #   min: 1.0
  #   max: 2.0

  # model.train_sys.steps.0.scheduler.cycle_decay:
  #   distribution: uniform
  #   min: 0.1
  #   max: 0.9

  # model.train_sys.steps.0.scheduler.cycle_limit:
  #   distribution: int_uniform
  #   min: 1
  #   max: 10


  # Loss Function Parameters
  model.train_sys.steps.0.loss.discrete_loss_fn._args_.0.path:
    values:
      - torch.nn.functional.cross_entropy
      - src.modules.training.loss.ce_focal_loss
      - src.modules.training.loss.ce_adaptive_loss
      - src.modules.training.loss.ce_rebalance_loss
      - src.modules.training.loss.ce_rebalanced_focal_loss